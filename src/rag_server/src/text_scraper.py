import os
import requests
import logging
import time
import json
import re
import hashlib
import random
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from urllib.robotparser import RobotFileParser
from requests.adapters import HTTPAdapter, Retry

class TextScraper:
    """Scrapes text data from URLs"""
    def __init__(self, base_url, output_dir="scraped_data", max_pages=1000):
        """
        Initialize the scraper with necessary attributes.
        :param base_url: The base URL to scrape.
        :param output_dir: Directory where scraped data will be stored.
        :param max_pages: Maximum number of pages to scrape.
        """
        self.base_url = base_url
        self.output_dir = output_dir
        self.max_pages = max_pages
        self.visited = set()
        self.to_visit = [base_url]
        self.failed_urls = {}
        self.scraped_data = []
        self.content_hashes = set()
        self.headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
        self.robot_parser = RobotFileParser()
        self.delay = 1  # Default delay, will update if `Crawl-Delay` exists
        self._setup_logging()
        self._setup_output_dir()
        self._initialize_robot_parser()
        self.session = self._setup_session()
        self._room_records = []  # room info

    def _setup_logging(self):
        """Set up logging for the scraper."""
        logging.basicConfig(
            filename="scraper.log",
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s"
        )

    def _setup_output_dir(self):
        """Create the base output directory if it doesn't exist."""
        os.makedirs(self.output_dir, exist_ok=True)

    def _initialize_robot_parser(self):
        """Parse the robots.txt file for the given domain."""
        parsed_url = urlparse(self.base_url)
        robots_url = urljoin(f"{parsed_url.scheme}://{parsed_url.netloc}", "/robots.txt")
        try:
            self.robot_parser.set_url(robots_url)
            self.robot_parser.read()  # downloads and parses the robots.txt file
            if self.robot_parser.crawl_delay("*"):
                self.delay = self.robot_parser.crawl_delay("*")
            logging.info(f"Robots.txt initialized from {robots_url}, Crawl Delay: {self.delay}s")
        except Exception as e:
            logging.warning(f"Failed to read robots.txt from {robots_url}: {e}")

    def _setup_session(self):
        """Configure a session with retries."""
        session = requests.Session()
        retries = Retry(
            total=3,  # total number of retries
            backoff_factor=1,  # exponential backoff factor, for example: 1, 2, 4 seconds
            status_forcelist=[500, 502, 503, 504]  # only retry on these status codes
        )
        session.mount("http://", HTTPAdapter(max_retries=retries))
        session.mount("https://", HTTPAdapter(max_retries=retries))
        return session

    def _can_scrape_url(self, url):
        """Check if the given URL can be scraped based on robots.txt rules."""
        return self.robot_parser.can_fetch("*", url)

    def _save_data(self):
        """Save scraped text data periodically to avoid data loss."""
        file_path = os.path.join(self.output_dir, "scraped_data.json")
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(self.scraped_data, f, ensure_ascii=False, indent=4)
        logging.info(f"Data saved to {file_path}.")

    def _save_rooms_json(self):
        """Save room numbers to rooms.json periodically to avoid data loss."""
        file_path = os.path.join(self.output_dir, "rooms.json")
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(self._room_records, f, ensure_ascii=False, indent=4)
        logging.info(f"Room info saved to {file_path}.")

    def _extract_text_content(self, soup, url=None):
        """Extract all text content from the page and room numbers if it's a team page (contains /team/ in the URL)."""
        # Normalize page text (turns weird whitespace like NBSP into spaces)
        
        try:
            
            paragraphs = [p.get_text(" ", strip=True) for p in soup.find_all("p")]
            headers = [h.get_text(" ", strip=True) for h in soup.find_all(re.compile('^h[1-6]$'))]
            contact_info = re.findall(r'\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}', soup.text) # already existing regex
            
            if url and self._extract_surname_from_url(url):
                text = soup.get_text(" ", strip=True)
                pattern = re.compile(r'\b(?:Raum|Room)\s*:?\s*([0-9]{1,3}\.[0-9]{1,3})\b', re.IGNORECASE) # regex generated by AI to match room numbers like Raum 2.116, Room:2.116 etc.
                room_numbers = list(dict.fromkeys(m.group(1) for m in pattern.finditer(text)))

            text_content = " ".join(paragraphs + headers + contact_info + room_numbers)

            extracted_text = {
                "paragraphs": paragraphs,
                "headers": headers,
                "phone_numbers": contact_info,
                "room_numbers": room_numbers,
                "hash": hashlib.md5(text_content.encode()).hexdigest()
            }
            return extracted_text
        
        except Exception as e:
            print(f"Error occured in _extract_text_content: {e}")
            return {}

    def _extract_surname_from_url(self, url: str) -> str:
        """Extract family name segments from teams URLs like '/institut/team/<familyname>/"""
        try:
            path_parts = [p for p in urlparse(url).path.split('/') if p]
            for i, part in enumerate(path_parts):
                if part.lower() == 'team' and i + 1 < len(path_parts):
                    surname_seg = path_parts[i + 1]
                    surname = re.sub(r'[^A-Za-z\-]', '', surname_seg) # regex generated by AI to keep only letters and dashes
                    return surname
        except Exception as e:
            print(f"Error occured in _extract_surname_from_url: {e}")
            return ''

    def _extract_full_name(self, soup: BeautifulSoup) -> str:
        """Extract the full name from teams page, which are in H1 tags."""
        try:
            h1 = soup.find("h1")
            if h1:
                return h1.get_text(strip=True)
        except Exception as e:
            print(f"Error occured in _extract_full_name: {e}")
        return ""

    def _extract_research_info(self, soup: BeautifulSoup) -> str:
        """Extract research info by finding a heading containing 'Research' (or German 'Forschung') and
        concatenating following paragraph/list texts until the next heading."""
        try:
            heading = None
            for tag in soup.find_all(re.compile('^h[1-6]$', re.I)):
                text = tag.get_text(" ", strip=True)
                if re.search(r"research|forschung|interests", text, flags=re.I):
                    heading = tag
                    break
            if not heading:
                # Fallback: look for any paragraph mentioning research
                p = soup.find("p", string=re.compile(r"research|forschung", re.I))
                return p.get_text(" ", strip=True) if p else ""

            parts = []
            for sib in heading.next_siblings:
                if getattr(sib, "name", None) and re.match(r"^h[1-6]$", sib.name, flags=re.I):
                    break  # stop at next heading
                if getattr(sib, "name", None) in {"p", "ul", "ol", "li", "div"}:
                    parts.append(sib.get_text(" ", strip=True))
            text = " ".join([p for p in parts if p])
            return text
        except Exception:
            return ""

    def scrape(self):
        """Main scraping function."""
        while self.to_visit and len(self.scraped_data) < self.max_pages:  # Limit to max_pages
            url = self.to_visit.pop(0)

            if url in self.visited or not self._can_scrape_url(url):
                continue  # Skip already visited or disallowed URLs

            try:
                response = self.session.get(url, headers=self.headers, timeout=10)
                content_type = response.headers.get("Content-Type", "")
                
                if "text/html" not in content_type:
                    logging.info(f"Skipping non-HTML content: {url}")
                    continue
                
                if response.status_code != 200:
                    logging.warning(f"Failed to fetch {url}: HTTP {response.status_code}")
                    self.failed_urls[url] = self.failed_urls.get(url, 0) + 1  # Count failures for each URL
                    if self.failed_urls[url] < 3:  # Retry up to 3 times
                        self.to_visit.append(url)
                    continue
                
                soup = BeautifulSoup(response.text, "html.parser")
                self.visited.add(url)

                # Extract text content
                page_data = self._extract_text_content(soup, url)
                if page_data["hash"] in self.content_hashes:
                    logging.info(f"Duplicate content skipped: {url}")
                    continue
                
                self.content_hashes.add(page_data["hash"])
                page_data["url"] = url
                self.scraped_data.append(page_data)

                # get family name, full names, room numbers, research info for team pages only NOT for geneic URLs
                surname = self._extract_surname_from_url(url)
                if surname:  # Only process if this is a team page
                    full_name = self._extract_full_name(soup)
                    research_info = self._extract_research_info(soup)
                    room_numbers = page_data.get("room_numbers")
                    if room_numbers and full_name:
                        rn = room_numbers[0] # to avoid multiple rooms, only consider 1st room number
                        # only add if not already present for this full_name + url, to prevent duplicates
                        exists = any(
                            rec["full_name"] == full_name and rec["urls"] == url
                            for rec in self._room_records
                        )
                        if not exists:
                            self._room_records.append({
                                "full_name": full_name,
                                "room_number": rn,
                                "urls": url,
                                "research_info": research_info
                            })

                # Save periodically
                if len(self.scraped_data) % 10 == 0:
                    self._save_data()
                    self._save_rooms_json()

                # Find and queue new links to visit
                for link in soup.find_all("a", href=True):
                    href = urljoin(url, link['href'])
                    if urlparse(href).netloc == urlparse(self.base_url).netloc and href not in self.visited:
                        self.to_visit.append(href)
                
                time.sleep(random.uniform(self.delay, self.delay * 2))
            except Exception as e:
                logging.error(f"Error scraping {url}: {e}")

        self._save_data()
        self._save_rooms_json()

    def _save_rooms(self):
        """Save collected (full_name, room_number, urls, research_info) records into rooms.csv."""
        if not self._room_records:
            return
        try:
            df = pd.DataFrame(self._room_records)
            # Keep only required columns in the specified order
            cols = ["full_name", "room_number", "urls", "research_info"]
            df = df[[c for c in cols if c in df.columns]]
            # Drop duplicates to keep CSV clean
            df = df.drop_duplicates(subset=["full_name", "room_number", "urls"], keep="first")
            file_path = os.path.join(self.output_dir, "rooms.csv")
            df.to_csv(file_path, index=False, encoding="utf-8")
            logging.info(f"Rooms CSV saved to {file_path}.")
        except Exception as e:
            logging.error(f"Failed to save rooms.csv: {e}")

    def save_to_csv(self):
        """Save scraped data to CSV."""
        file_path = os.path.join(self.output_dir, "scraped_data.csv")
        df = pd.DataFrame(self.scraped_data)
        df.to_csv(file_path, index=False, encoding="utf-8")
        logging.info(f"Scraped data saved to {file_path}.")
        # Also save rooms once more at the end
        self._save_rooms()

# Usage
def run_default_scraping():
    base_url = "https://www.ias.uni-stuttgart.de/"
    scraper = TextScraper(base_url=base_url, output_dir="ias_scraped_data", max_pages=500)
    scraper.scrape()
    scraper.save_to_csv()
    
def save_to_chromadb(config):
    # save to chroma db
    OUTPUT_DIR = config.OUTPUT_DIR
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    vector_db_handler = DatabaseHandler(path=config.CHROMA_PATH, model_name=config.EMBEDDING_MODEL_NAME)
    data_processor = DocumentProcessor(config.CSV_FILE_PATH)
    text_chunks, metadatas = data_processor.get_combined_chunks_with_rooms(config.ROOMS_CSV_PATH)
    vector_db_handler.store_documents(text_chunks, metadatas)

if __name__ == "__main__":
    
    from databaseHandler import DatabaseHandler
    from documentProcessor import DocumentProcessor
    import os, sys
    
    # go up one folder
    parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    sys.path.append(parent_dir)
    
    import config
    
    # scrape and save to JSON/CSV
    # start_time = time.time()
    # print("Starting scraping process...")
    # run_default_scraping()
    # print("Scraping process completed.")
    # end_time = time.time()
    # print(f"Total scraping time: {end_time - start_time:.2f} seconds")
    
    # save to ChromaDB
    start_time = time.time()
    print("Saving to ChromaDB...")
    save_to_chromadb(config=config)
    print("Data saved to ChromaDB.")
    end_time = time.time()
    print(f"Total time to save to ChromaDB: {end_time - start_time:.2f} seconds")
