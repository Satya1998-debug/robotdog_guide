import openai
import os
import requests
from langchain_ollama import ChatOllama
class AnswerGenerator:
    """Generates answers based on input prompts using a language model."""

    def __init__(self, db_handler, logger):
        """Initializes the AnswerGenerator with a specified dbHandler.

        Args:
            db_handler: The database handler to query for relevant documents.
        """
    
        self.db_handler = db_handler
        self.logger = logger

    def generate_ollama(self, query, memory=None, use_mcp=False):
        """Generates a response for a given query and optionally uses memory for continuous conversation.

        Args:
            query (str): Input Query.
            memory (Optional): Memory instance to obtain historical conversation context.

        Returns:
            str: Answer generated by the model.
        """
        self.logger.info(f"generate_ollama is called.")   
        docs = self.db_handler.query(query)
        context = "\n\n".join([doc["content"] if isinstance(doc, dict) else str(doc) for doc in docs]) if docs else "No relevant documents found."
        systemPrompt = "You are the robbot dog controller and a helpful assistant."
        
        if memory:
            context = "\n".join(memory.get_context()) + "\n" + context
            
        if use_mcp:
            self.logger.info("ChatOllama is used.")
            userPrompt = f"Keep your answers short and to the point. Context:\n{context}\n\nQuestion: {query}\n\nAnswer:"
            messages = [
                {"role": "system", "content": systemPrompt},
                {"role": "user", "content": userPrompt}
            ]

            rag_llm = ChatOllama(
                model="qwen3:8b",
                base_url="http://localhost:11434",
                validate_model_on_init=True,
                temperature=0.2,
            )
            response = rag_llm.invoke(messages).content
            
        else:
            self.logger.info("Ollama API is used.")
            userPrompt = f"{systemPrompt}\nKeep your answers short and to the point. Context:\n{context}\n\nQuestion: {query}\n\nAnswer:"
            response = requests.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": "llama3:8b",
                    "prompt": userPrompt,
                    "stream": False
                }
            )
            response = response.json()["response"]

        return response
